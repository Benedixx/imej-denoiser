{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import collections.abc\n",
    "from IPython.display import clear_output            \n",
    "\n",
    "collections.Iterable = collections.abc.Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnovebritito\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\project\\imej-denoiser\\wandb\\run-20241225_213219-ndv4rqr8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/novebritito/imej-denoiser/runs/ndv4rqr8' target=\"_blank\">DAE</a></strong> to <a href='https://wandb.ai/novebritito/imej-denoiser' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/novebritito/imej-denoiser' target=\"_blank\">https://wandb.ai/novebritito/imej-denoiser</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/novebritito/imej-denoiser/runs/ndv4rqr8' target=\"_blank\">https://wandb.ai/novebritito/imej-denoiser/runs/ndv4rqr8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/novebritito/imej-denoiser/runs/ndv4rqr8?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x20c47834170>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = 'data'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 32\n",
    "wandb.init(project='imej-denoiser', name='DAE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No class for this dataset\n",
    "class NonClassDataLoader(Dataset):\n",
    "    def __init__(self, root_dir, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_paths = glob(os.path.join(root_dir, '*.jpg')) + \\\n",
    "                   glob(os.path.join(root_dir, '*.jpeg')) + \\\n",
    "                   glob(os.path.join(root_dir, '*.png'))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the named functions\n",
    "def add_gaussian_noise(x):\n",
    "    return x + 0.08 * torch.randn_like(x)\n",
    "\n",
    "def normalize_tensor(x):\n",
    "    return torch.clamp(x, 0., 1.)\n",
    "\n",
    "def resize_tensor(x):\n",
    "    return F.interpolate(x.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "# Update the transform with named functions\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)), # resize image\n",
    "    transforms.ToTensor(), # convert image to tensor\n",
    "    transforms.Lambda(add_gaussian_noise), # add gaussian noise\n",
    "    transforms.Lambda(normalize_tensor), # normalize tensors to [0, 1]\n",
    "])\n",
    "\n",
    "# Load data from folder\n",
    "noised_data = NonClassDataLoader(root_dir=DATA_PATH, transforms=transform)\n",
    "clean_data = NonClassDataLoader(root_dir=DATA_PATH, transforms=transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()]))\n",
    "\n",
    "train_data = DataLoader(\n",
    "    dataset=noised_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "clean_data = DataLoader(\n",
    "    dataset=clean_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAE (Denoising AutoEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, input_channels=3):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1),  #  256x256 -> 128x128\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),              # 128x128 -> 64x64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),             # 64x64 -> 32x32\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),            # 32x32 -> 16x16\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),  # 16x16 -> 32x32\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),   # 32x32 -> 64x64\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # 64x64 -> 128x128\n",
    "            nn.Sigmoid(),  # Normalize output between 0 and 1\n",
    "            nn.ConvTranspose2d(32, input_channels, kernel_size=3, stride=2, padding=1, output_padding=1),  # 128x128 -> 256x256\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor):\n",
    "    \"\"\"Function to denormalize tensor values back to [0, 1]\"\"\"\n",
    "    return tensor.clamp(0., 1.)\n",
    "\n",
    "def plot_images(noisy, denoised, epoch, loss):\n",
    "    \"\"\"Plot images for each epoch\"\"\"\n",
    "    noisy = denormalize(noisy).cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    denoised = denormalize(denoised).cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "    # Clear the figure to avoid overlapping\n",
    "    plt.clf()\n",
    "    \n",
    "    # Create a plot to show noisy, denoised, and original images\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # title for the plot\n",
    "    plt.suptitle(f\"Epoch: {epoch},  loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Noisy image   \n",
    "    axes[0].imshow(noisy[0])  \n",
    "    axes[0].set_title(\"Noisy Image\")\n",
    "    \n",
    "    # Denoised image (model output)\n",
    "    axes[1].imshow(denoised[0])  \n",
    "    axes[1].set_title(\"Denoised Image\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    clear_output(wait=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenoisingAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (5): Sigmoid()\n",
       "    (6): ConvTranspose2d(32, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 300\n",
    "dae_model = DenoisingAutoencoder(input_channels=3).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(dae_model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "criterion = nn.MSELoss()\n",
    "dae_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇██</td></tr><tr><td>loss</td><td>▇█▇▇▆▅▄▄▄▃▃▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>299</td></tr><tr><td>loss</td><td>0.00199</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DAE</strong> at: <a href='https://wandb.ai/novebritito/imej-denoiser/runs/ndv4rqr8' target=\"_blank\">https://wandb.ai/novebritito/imej-denoiser/runs/ndv4rqr8</a><br> View project at: <a href='https://wandb.ai/novebritito/imej-denoiser' target=\"_blank\">https://wandb.ai/novebritito/imej-denoiser</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 300 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241225_213219-ndv4rqr8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('START TRAINING')\n",
    "for epoch in range(epochs):\n",
    "    for noisy, clean in zip(train_data, clean_data):\n",
    "        noisy = noisy.to(DEVICE)\n",
    "        clean = clean.to(DEVICE)\n",
    "        \n",
    "        denoised_img = dae_model(noisy)\n",
    "        loss = criterion(denoised_img, clean)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # randomly select a noisy image\n",
    "        noisy_img = random.choice(noised_data)\n",
    "        noisy_img = noisy_img.unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        # denoise the noisy image\n",
    "        denoised_img = dae_model(noisy_img)\n",
    "        \n",
    "        # plot the noisy and denoised image\n",
    "        plot_images(noisy_img, denoised_img, epoch, loss)        \n",
    "        wandb.log({\n",
    "            'loss': loss.item(),\n",
    "            'epoch': epoch,\n",
    "            'denoised_img': wandb.Image(denoised_img[0].cpu().numpy().transpose(1, 2, 0)),\n",
    "        })\n",
    "        \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(dae_model.state_dict(), 'vae_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
